{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return np.sum(np.square(x - y)) / x.size\n",
    "    \n",
    "    def backward(self):\n",
    "        dx = 2 * (self.x - self.y) / self.x.size\n",
    "        return dx, -dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "np.set_printoptions(precision=6, suppress=True, linewidth=80)\n",
    "\n",
    "x_numpy = np.random.random(27)\n",
    "y_numpy = np.random.random(27)\n",
    "x_torch = torch.tensor(x_numpy, requires_grad=True)\n",
    "y_torch = torch.tensor(y_numpy, requires_grad=True)\n",
    "\n",
    "loss_func_numpy = MSELoss()\n",
    "loss_func_torch = torch.nn.MSELoss().float()\n",
    "\n",
    "loss_numpy = loss_func_numpy(x_numpy, y_numpy)\n",
    "loss_torch = loss_func_torch(x_torch, y_torch)\n",
    "\n",
    "loss_torch.backward()\n",
    "dx_numpy, dy_numpy = loss_func_numpy.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11696011656631419\n",
      "0.1169601165663142\n",
      "----------\n",
      "[ 0.034682 -0.000561 -0.029935  0.034016  0.021168 -0.000575  0.03608\n",
      "  0.019185  0.012494 -0.002536 -0.040756 -0.015934 -0.004686 -0.041798\n",
      "  0.02092   0.031164 -0.01721  -0.051175  0.020822  0.003614 -0.026012\n",
      "  0.02444   0.008264  0.036326 -0.007696 -0.020748 -0.013576]\n",
      "[ 0.034682 -0.000561 -0.029935  0.034016  0.021168 -0.000575  0.03608\n",
      "  0.019185  0.012494 -0.002536 -0.040756 -0.015934 -0.004686 -0.041798\n",
      "  0.02092   0.031164 -0.01721  -0.051175  0.020822  0.003614 -0.026012\n",
      "  0.02444   0.008264  0.036326 -0.007696 -0.020748 -0.013576]\n",
      "----------\n",
      "[-0.034682  0.000561  0.029935 -0.034016 -0.021168  0.000575 -0.03608\n",
      " -0.019185 -0.012494  0.002536  0.040756  0.015934  0.004686  0.041798\n",
      " -0.02092  -0.031164  0.01721   0.051175 -0.020822 -0.003614  0.026012\n",
      " -0.02444  -0.008264 -0.036326  0.007696  0.020748  0.013576]\n",
      "[-0.034682  0.000561  0.029935 -0.034016 -0.021168  0.000575 -0.03608\n",
      " -0.019185 -0.012494  0.002536  0.040756  0.015934  0.004686  0.041798\n",
      " -0.02092  -0.031164  0.01721   0.051175 -0.020822 -0.003614  0.026012\n",
      " -0.02444  -0.008264 -0.036326  0.007696  0.020748  0.013576]\n"
     ]
    }
   ],
   "source": [
    "print(loss_numpy)\n",
    "print(loss_torch.data.numpy())\n",
    "print(\"----------\")\n",
    "print(dx_numpy)\n",
    "print(x_torch.grad.numpy())\n",
    "print(\"----------\")\n",
    "print(dy_numpy)\n",
    "print(y_torch.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entropy:\n",
    "    def __init__(self):\n",
    "        self.nx = None\n",
    "        self.ny = None\n",
    "        self.ndx = None\n",
    "        \n",
    "    def loss(self, nx, ny):\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        loss = np.sum(- ny * np.log(nx))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        self.ndx = - self.ny / self.nx\n",
    "        return self.ndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=120)\n",
    "\n",
    "entropy = Entropy()\n",
    "\n",
    "x = np.random.random([5, 10])\n",
    "y = np.random.random([5, 10])\n",
    "x_tensor = torch.tensor(x, requires_grad=True)\n",
    "y_tensor = torch.tensor(y, requires_grad=True)\n",
    "\n",
    "loss_numpy = entropy.loss(x, y)\n",
    "grad_numpy = entropy.backward()\n",
    "\n",
    "loss_tensor = (- y_tensor * torch.log(x_tensor)).sum()\n",
    "loss_tensor.backward()\n",
    "grad_tensor = x_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Loss : 22.609416116382963\n",
      "PyTorch Loss : 22.60941611638296\n",
      "\n",
      "Python dx :\n",
      "[[ -0.173  -2.888  -2.658  -0.989  -0.476  -0.719  -0.425  -0.995  -1.82   -1.302]\n",
      " [ -1.95   -0.804  -1.425 -11.306  -2.116  -0.113  -4.185  -1.389  -0.365  -1.076]\n",
      " [ -0.151  -1.042  -0.866  -1.184  -0.022  -1.841  -1.539  -0.696  -0.521  -1.102]\n",
      " [ -3.461  -1.596  -1.287  -0.788  -2.173  -2.695  -0.838  -0.049  -0.323  -0.793]\n",
      " [ -1.13   -8.609  -1.122  -1.838  -0.685  -2.762  -0.313  -0.405  -0.464  -0.56 ]]\n",
      "\n",
      "PyTorch dx :\n",
      "[[ -0.173  -2.888  -2.658  -0.989  -0.476  -0.719  -0.425  -0.995  -1.82   -1.302]\n",
      " [ -1.95   -0.804  -1.425 -11.306  -2.116  -0.113  -4.185  -1.389  -0.365  -1.076]\n",
      " [ -0.151  -1.042  -0.866  -1.184  -0.022  -1.841  -1.539  -0.696  -0.521  -1.102]\n",
      " [ -3.461  -1.596  -1.287  -0.788  -2.173  -2.695  -0.838  -0.049  -0.323  -0.793]\n",
      " [ -1.13   -8.609  -1.122  -1.838  -0.685  -2.762  -0.313  -0.405  -0.464  -0.56 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Python Loss :\", loss_numpy)\n",
    "print(\"PyTorch Loss :\", loss_tensor.data.numpy())\n",
    "\n",
    "print(\"\\nPython dx :\")\n",
    "print(grad_numpy)\n",
    "print(\"\\nPyTorch dx :\")\n",
    "print(grad_tensor.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.softmax = None\n",
    "        self.grad = None\n",
    "        self.dnx = None\n",
    "\n",
    "    def __call__(self, nx):\n",
    "        shifted_x = nx - np.max(nx)\n",
    "        ex = np.exp(shifted_x)\n",
    "        sum_ex = np.sum(ex)\n",
    "        self.softmax = ex / sum_ex\n",
    "        return self.softmax\n",
    "\n",
    "    def get_grad(self):\n",
    "        self.grad = self.softmax[:, np.newaxis] * self.softmax[np.newaxis, :]\n",
    "        for i in range(len(self.grad)):\n",
    "            self.grad[i, i] -= self.softmax[i]\n",
    "        self.grad = - self.grad\n",
    "        return self.grad\n",
    "\n",
    "    def backward(self, dl):\n",
    "        self.get_grad()\n",
    "        self.dnx = np.sum(self.grad * dl, axis=1)\n",
    "        return self.dnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=120)\n",
    "\n",
    "d_loss = np.array([11, 12, 13, 14, 15, 16, 17, 18, 19], dtype=float)\n",
    "d_loss_tensor = torch.tensor(d_loss, requires_grad=True)\n",
    "\n",
    "softmax_numpy = Softmax()\n",
    "x_numpy = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float)\n",
    "soft_numpy = softmax_numpy(x_numpy)\n",
    "x_grad_numpy = softmax_numpy.backward(d_loss)\n",
    "\n",
    "x_tensor = torch.tensor(x_numpy, requires_grad=True)\n",
    "soft_tensor = torch.nn.functional.softmax(x_tensor, dim=0)\n",
    "soft_tensor.backward(d_loss_tensor)\n",
    "x_grad_tensor = x_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00021208 0.00057649 0.00156706 0.00425972 0.01157912 0.03147531 0.08555877 0.23257286 0.63219858]\n",
      "[0.00021208 0.00057649 0.00156706 0.00425972 0.01157912 0.03147531 0.08555877 0.23257286 0.63219858]\n",
      "\n",
      "[-0.00157344 -0.00370057 -0.00849213 -0.01882428 -0.03959057 -0.07614301 -0.12141937 -0.09747922  0.36722258]\n",
      "[-0.00157344 -0.00370057 -0.00849213 -0.01882428 -0.03959057 -0.07614301 -0.12141937 -0.09747922  0.36722258]\n"
     ]
    }
   ],
   "source": [
    "print(soft_numpy)\n",
    "print(soft_tensor.data.numpy())\n",
    "print()\n",
    "print(x_grad_numpy)\n",
    "print(x_grad_tensor.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax + cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmaxEntropy:\n",
    "    def __init__(self):\n",
    "        self.nx = None\n",
    "        self.ny = None\n",
    "        self.ndx = None\n",
    "        self.softmax = None\n",
    "        self.entropy = None\n",
    "        self.loss = None\n",
    "        \n",
    "    def __call__(self, nx, ny):\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        shifted_x = nx - np.max(nx)\n",
    "        ex = np.exp(shifted_x)\n",
    "        sum_ex = np.sum(ex)\n",
    "        self.softmax = ex / sum_ex\n",
    "        self.entropy = - np.log(self.softmax) * ny\n",
    "        self.loss = np.sum(self.entropy)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        self.ndx = self.softmax.copy() * np.sum(self.ny)\n",
    "        self.ndx -= self.ny\n",
    "        return self.ndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.063482775853203\n",
      "14.063482775853203\n",
      "\n",
      "[-0.09904564 -0.19740579 -0.29294821 -0.38083126 -0.44789396 -0.45836109 -0.31498552  0.24657787  1.9448936 ]\n",
      "[-0.09904564 -0.19740579 -0.29294821 -0.38083126 -0.44789396 -0.45836109 -0.31498552  0.24657787  1.9448936 ]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=8, suppress=True, linewidth=120)\n",
    "\n",
    "x_numpy = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=np.float)\n",
    "y_numpy = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=np.float)\n",
    "x_tensor = torch.tensor(x_numpy, requires_grad=True)\n",
    "y_tensor = torch.tensor(y_numpy)\n",
    "\n",
    "softmax_entropy_loss = softmaxEntropy()\n",
    "loss_numpy = softmax_entropy_loss(x_numpy, y_numpy)\n",
    "dx_numpy = softmax_entropy_loss.backward()\n",
    "\n",
    "log_softmax_loss = torch.nn.LogSoftmax(dim=0)\n",
    "log_softmax_tensor = log_softmax_loss(x_tensor)\n",
    "entropy_tensor = - log_softmax_tensor * y_tensor\n",
    "loss_tensor = entropy_tensor.sum()\n",
    "loss_tensor.backward()\n",
    "dx_tensor = x_tensor.grad\n",
    "\n",
    "print(loss_numpy)\n",
    "print(loss_tensor.data.numpy())\n",
    "print()\n",
    "print(dx_numpy)\n",
    "print(dx_tensor.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
