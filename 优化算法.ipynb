{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_grad\n",
      "[[2.29158508 1.95058016 2.25510989 2.56106592 2.06111261]\n",
      " [1.25926989 1.57975955 1.58000814 1.67232418 1.86585193]\n",
      " [2.20280346 2.10071483 2.20099271 1.84145669 1.87640346]\n",
      " [2.17063112 2.22953686 2.53307273 2.04808866 2.35552527]]\n",
      "[[1.89687006 1.89687006 1.89687006 1.89687006 1.89687006]\n",
      " [1.10136331 1.10136331 1.10136331 1.10136331 1.10136331]\n",
      " [1.55079367 1.55079367 1.55079367 1.55079367 1.55079367]\n",
      " [1.96519422 1.96519422 1.96519422 1.96519422 1.96519422]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "np.set_printoptions(8, suppress=True)\n",
    "\n",
    "x_numpy = np.random.random((3, 4)).astype(np.double)\n",
    "x_torch = torch.tensor(x_numpy, requires_grad=True)\n",
    "x_torch2 = torch.tensor(x_numpy, requires_grad=True)\n",
    "\n",
    "w_numpy = np.random.random((4, 5)).astype(np.double)\n",
    "w_torch = torch.tensor(w_numpy, requires_grad=True)\n",
    "w_torch2 = torch.tensor(w_numpy, requires_grad=True)\n",
    "\n",
    "lr = 0.01\n",
    "weight_decay = 0.9\n",
    "sgd = torch.optim.SGD([w_torch], lr=lr, weight_decay=0)\n",
    "sgd2 = torch.optim.SGD([w_torch2], lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "y_torch = torch.matmul(x_torch, w_torch)\n",
    "y_torch2 = torch.matmul(x_torch2, w_torch2)\n",
    "\n",
    "loss = y_torch.sum()\n",
    "loss2 = y_torch2.sum()\n",
    "\n",
    "sgd.zero_grad()\n",
    "sgd2.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss2.backward()\n",
    "\n",
    "sgd.step()\n",
    "sgd2.step()\n",
    "\n",
    "w_grad = w_torch.grad.data.numpy()\n",
    "w_grad2 = w_torch2.grad.data.numpy()\n",
    "\n",
    "print(\"check_grad\")\n",
    "print(w_grad + weight_decay * w_numpy)\n",
    "print(w_grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def __call__(self, params, grads):\n",
    "        params -= self.lr * grads\n",
    "    \n",
    "    \n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def __call__(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.v = self.momentum * self.v + grads\n",
    "        params -= self.lr * self.v\n",
    "        \n",
    "\n",
    "class RMSProp:\n",
    "    def __init__(self, lr=0.01, alpha=0.9, eps=1e-08):\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.v = None\n",
    "\n",
    "    def __call__(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "\n",
    "        self.v = self.alpha * self.v\n",
    "        self.v += (1 - self.alpha) * np.square(grads)\n",
    "        eta = self.lr / (np.sqrt(self.v) + self.eps)\n",
    "        params -= eta * grads\n",
    "        \n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.01, betas=(0.9, 0.999), eps=1e-08):\n",
    "        self.lr = lr\n",
    "        self.beta1 = betas[0]\n",
    "        self.beta2 = betas[1]\n",
    "        self.eps = eps\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.n = 0\n",
    "        \n",
    "    def __call__(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "            \n",
    "        self.n += 1\n",
    "        \n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grads\n",
    "        \n",
    "        Mt = self.m / (1 - np.power(self.beta1, self.n))\n",
    "        Vt = self.v / (1 - np.power(self.beta2, self.n))\n",
    "        \n",
    "        eta = self.lr / (np.sqrt(Vt) + self.eps)\n",
    "        params -= eta * Mt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_optim(optim_numpy, optim_torch, p, p_torch):\n",
    "    \"\"\"\n",
    "    check with y = p * x^2\n",
    "    optim param p\n",
    "    \"\"\"\n",
    "    x_size = 5\n",
    "    x = np.random.random(x_size)\n",
    "    x_torch = torch.tensor(x, requires_grad=True)\n",
    "    \n",
    "    dxi_numpy_list = []\n",
    "    for i in range(x_size):\n",
    "        yi_numpy = p * x[i] ** 2\n",
    "        dxi_numpy = 2 * p * x[i]\n",
    "        dxi_numpy_list.append(dxi_numpy)\n",
    "\n",
    "        da = x[i] ** 2\n",
    "        optim_numpy(p, da)\n",
    "        \n",
    "    for i in range(x_size):\n",
    "        yi_torch = p_torch * x_torch[i] ** 2\n",
    "        optim_torch.zero_grad()\n",
    "        yi_torch.backward()\n",
    "        optim_torch.step()\n",
    "\n",
    "    print(np.array(dxi_numpy_list))\n",
    "    print(x_torch.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 检查SGD ---\n",
      "[1.671526045435 0.658974920984 0.518721027022 1.254968104394 1.594004424417]\n",
      "[1.671526045435 0.658974920984 0.518721027022 1.254968104394 1.594004424417]\n",
      "--- 检查Momentum ---\n",
      "[1.015455504299 2.318718975892 1.46525696166  0.886671018481 0.600349866658]\n",
      "[1.015455504299 2.318718975892 1.46525696166  0.886671018481 0.600349866658]\n",
      "--- 检查RMSProp ---\n",
      "[0.823627238762 1.288627900967 0.503750904131 0.055347017535 0.367439505846]\n",
      "[0.823627238762 1.288627900967 0.503750904131 0.055347017535 0.367439505846]\n",
      "--- 检查Adam ---\n",
      "[1.771188973757 0.411080990314 0.377231396585 1.099067114457 1.051522639875]\n",
      "[1.771188973757 0.40154869823  0.361003567239 1.031188611276 0.957820725513]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "np.set_printoptions(precision=12, suppress=True, linewidth=80)\n",
    "\n",
    "print(\"--- 检查SGD ---\")\n",
    "a_numpy = np.array(1.2)\n",
    "a_torch = torch.tensor(a_numpy, requires_grad=True)\n",
    "sgd_numpy = SGD(0.1)\n",
    "sgd_torch = torch.optim.SGD([a_torch], lr=0.1)\n",
    "check_optim(sgd_numpy, sgd_torch, a_numpy, a_torch)\n",
    "\n",
    "print(\"--- 检查Momentum ---\")\n",
    "a_numpy = np.array(1.2)\n",
    "a_torch = torch.tensor(a_numpy, requires_grad=True)\n",
    "momentum_numpy = Momentum(0.1, 0.9)\n",
    "momentum_torch = torch.optim.SGD([a_torch], lr=0.1, momentum=0.9)\n",
    "check_optim(momentum_numpy, momentum_torch, a_numpy, a_torch)\n",
    "\n",
    "print(\"--- 检查RMSProp ---\")\n",
    "a_numpy = np.array(1.2)\n",
    "a_torch = torch.tensor(a_numpy, requires_grad=True)\n",
    "rms_numpy = RMSProp(0.1, 0.9, eps=1e-08)\n",
    "rms_torch = torch.optim.RMSprop([a_torch], lr=0.1, alpha=0.9)\n",
    "check_optim(rms_numpy, rms_torch, a_numpy, a_torch)\n",
    "\n",
    "print(\"--- 检查Adam ---\")\n",
    "a_numpy = np.array(1.2)\n",
    "a_torch = torch.tensor(a_numpy, requires_grad=True)\n",
    "adam_numpy = Adam(lr=0.1, betas=(0.9, 0.99), eps=0.001)\n",
    "adam_torch = torch.optim.Adam([a_torch], lr=0.1, betas=(0.9, 0.99), eps=0.001)\n",
    "check_optim(adam_numpy, adam_torch, a_numpy, a_torch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_numpy \n",
      " [[ 1.65328862  0.60845792  0.51520137  1.06970231  1.47430633]\n",
      " [ 0.31989274  1.049903    0.94450061  0.89737849  0.33620922]\n",
      " [-0.06997848  0.88993446  0.713664   -0.13401027  0.3568146 ]]\n",
      "output_tensor \n",
      " [[ 1.65328862  0.60845792  0.51520137  1.06970231  1.47430633]\n",
      " [ 0.31989274  1.049903    0.94450061  0.89737849  0.33620922]\n",
      " [-0.06997848  0.88993446  0.713664   -0.13401027  0.3568146 ]]\n",
      "dx_numpy \n",
      " [[ 0.14897849 -0.00280487 -0.19168465 -0.12787269 -0.00214988]\n",
      " [-0.65806077 -0.00492716 -0.16475823  0.14902235 -0.12571213]\n",
      " [ 0.50908229  0.00773203  0.35644287 -0.02114966  0.127862  ]]\n",
      "dx_tensor \n",
      " [[ 0.14897849 -0.00280487 -0.19168465 -0.12787269 -0.00214988]\n",
      " [-0.65806077 -0.00492716 -0.16475823  0.14902235 -0.12571213]\n",
      " [ 0.50908229  0.00773203  0.35644287 -0.02114966  0.127862  ]]\n",
      "dw_numpy \n",
      " [ 0.10859242  0.09332669  0.21318384 -0.80395148  0.23776771]\n",
      "dw_tensor \n",
      " [ 0.10859242  0.09332669  0.21318384 -0.80395148  0.23776771]\n",
      "db_numpy \n",
      " [0.72732508 1.22184114 1.55251516 1.73155916 1.55864309]\n",
      "db_tensor \n",
      " [0.72732508 1.22184114 1.55251516 1.73155916 1.55864309]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self):\n",
    "        self.eps = 1e-5\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "\n",
    "        self.num = None\n",
    "        self.std = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.num = np.shape(x)[0]\n",
    "        mean = np.mean(x, axis=0, keepdims=True)\n",
    "        var = np.var(x, axis=0, keepdims=True)\n",
    "        self.sqrt = np.sqrt(var + self.eps)\n",
    "        self.std = (x - mean) / self.sqrt\n",
    "        out = self.std * self.weight + self.bias\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_loss):\n",
    "        std_t = self.std.T\n",
    "        shape_t = np.shape(std_t)\n",
    "        r = np.zeros([shape_t[0], shape_t[1], shape_t[1]])\n",
    "        shift_eye = np.eye(shape_t[1]) * shape_t[1] - 1\n",
    "        for i in range(shape_t[0]):\n",
    "            r[i] = std_t[i][:, np.newaxis] * std_t[i][np.newaxis, :]\n",
    "            r[i] = shift_eye - r[i]\n",
    "\n",
    "        u = self.weight / shape_t[1] / self.sqrt\n",
    "        u = u.T\n",
    "        y = r * u[:, np.newaxis]\n",
    "\n",
    "        dx = np.zeros(shape_t)\n",
    "        for i in range(shape_t[0]):\n",
    "            dx[i] = np.dot(d_loss.T[i], y[i])\n",
    "        dx = dx.T\n",
    "\n",
    "        self.dw = np.sum(self.std * d_loss, axis=0)\n",
    "        self.db = np.sum(d_loss, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=120)\n",
    "np.random.seed(123)\n",
    "torch.random.manual_seed(123)\n",
    "\n",
    "x_numpy = np.array(np.random.random((3, 5)), dtype=np.float64)\n",
    "weight_numpy = np.array(np.random.random((5,)), dtype=np.float64)\n",
    "bias_numpy = np.array(np.random.random((5,)), dtype=np.float64)\n",
    "d_loss_numpy = np.array(np.random.random((3, 5)), dtype=np.float64)\n",
    "\n",
    "x_tensor = torch.tensor(x_numpy, requires_grad=True)\n",
    "weight_tensor = torch.tensor(weight_numpy, requires_grad=True)\n",
    "bias_tensor = torch.tensor(bias_numpy, requires_grad=True)\n",
    "d_loss_tensor = torch.tensor(d_loss_numpy, requires_grad=True)\n",
    "\n",
    "batch_norm_numpy = BatchNorm1d()\n",
    "batch_norm_numpy.weight = weight_numpy\n",
    "batch_norm_numpy.bias = bias_numpy\n",
    "\n",
    "batch_norm_tensor = torch.nn.BatchNorm1d(5).double()\n",
    "batch_norm_tensor.weight = torch.nn.Parameter(weight_tensor, requires_grad=True)\n",
    "batch_norm_tensor.bias = torch.nn.Parameter(bias_tensor, requires_grad=True)\n",
    "\n",
    "output_numpy = batch_norm_numpy(x_numpy)\n",
    "output_tensor = batch_norm_tensor(x_tensor)\n",
    "output_tensor.backward(d_loss_tensor)\n",
    "\n",
    "dx_numpy = batch_norm_numpy.backward(d_loss_numpy)\n",
    "dx_tensor = x_tensor.grad\n",
    "\n",
    "dw_numpy = batch_norm_numpy.dw\n",
    "dw_tensor = batch_norm_tensor.weight.grad\n",
    "\n",
    "db_numpy = batch_norm_numpy.db\n",
    "db_tensor = batch_norm_tensor.bias.grad\n",
    "\n",
    "print(\"output_numpy \\n\", output_numpy)\n",
    "print(\"output_tensor \\n\", output_tensor.data.numpy())\n",
    "\n",
    "print(\"dx_numpy \\n\", dx_numpy)\n",
    "print(\"dx_tensor \\n\", dx_tensor.data.numpy())\n",
    "\n",
    "print(\"dw_numpy \\n\", dw_numpy)\n",
    "print(\"dw_tensor \\n\", dw_tensor.data.numpy())\n",
    "\n",
    "print(\"db_numpy \\n\", db_numpy)\n",
    "print(\"db_tensor \\n\", db_tensor.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, train=True, momentum=0.1, eps=1e-5):\n",
    "        self.train = train\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "\n",
    "        self.std = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "        self.sqrt = None\n",
    "        self.std = None\n",
    "\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.train is True:\n",
    "            mean = np.mean(x, axis=0, keepdims=True)\n",
    "            var = np.var(x, axis=0, keepdims=True)\n",
    "            sqrt = np.sqrt(var + self.eps)\n",
    "            std = (x - mean) / sqrt\n",
    "            self.sqrt = sqrt\n",
    "            self.std = std\n",
    "\n",
    "            if self.running_mean is None:\n",
    "                self.running_mean = np.zeros_like(mean)\n",
    "                self.running_var = np.ones_like(var)\n",
    "\n",
    "            num = np.shape(x)[0]\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean\n",
    "            self.running_mean += self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var\n",
    "            self.running_var += self.momentum * var * num / (num - 1)\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "            sqrt = np.sqrt(var + self.eps)\n",
    "            std = (x - mean) / sqrt\n",
    "\n",
    "        out = std * self.weight + self.bias\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_loss):\n",
    "        std_t = self.std.T\n",
    "        shape_t = np.shape(std_t)\n",
    "        r = np.zeros([shape_t[0], shape_t[1], shape_t[1]])\n",
    "        shift_eye = np.eye(shape_t[1]) * shape_t[1] - 1\n",
    "        for i in range(shape_t[0]):\n",
    "            r[i] = std_t[i][:, np.newaxis] * std_t[i][np.newaxis, :]\n",
    "            r[i] = shift_eye - r[i]\n",
    "\n",
    "        u = self.weight / shape_t[1] / self.sqrt\n",
    "        u = u.T\n",
    "        y = r * u[:, np.newaxis]\n",
    "\n",
    "        dx = np.zeros(shape_t)\n",
    "        for i in range(shape_t[0]):\n",
    "            dx[i] = np.dot(d_loss.T[i], y[i])\n",
    "        dx = dx.T\n",
    "\n",
    "        self.dw = np.sum(self.std * d_loss, axis=0)\n",
    "        self.db = np.sum(d_loss, axis=0)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_numpy\n",
      "[[ 0.46641115  0.18851114 -0.57237571  0.99333476  1.2092339 ]\n",
      " [ 0.01389322  1.35696463  1.83911376  0.82492271 -0.10234434]\n",
      " [-0.11841836  0.93354663  0.54244234 -0.18305345 -0.07859806]]\n",
      "[[ 0.36607296  0.34287603 -0.19635263  0.74842757 -0.41712586]\n",
      " [ 0.21899343  1.48596725  1.99143145  1.05458233  1.08250983]\n",
      " [-0.22318039  0.65017912  0.01410157 -0.16780589  0.36290753]]\n",
      "[[-0.16707033  1.17972157  0.14300413  0.26912336 -0.20601642]\n",
      " [ 0.08560938  1.15577836  1.97239442  1.27262803  0.03626041]\n",
      " [ 0.44334695  0.14352248 -0.30621817  0.09345263  1.19804751]]\n",
      "output_tensor\n",
      "[[ 0.46641115  0.18851114 -0.57237571  0.99333476  1.2092339 ]\n",
      " [ 0.01389322  1.35696463  1.83911376  0.82492271 -0.10234434]\n",
      " [-0.11841836  0.93354663  0.54244234 -0.18305345 -0.07859806]]\n",
      "[[ 0.36607296  0.34287603 -0.19635263  0.74842757 -0.41712586]\n",
      " [ 0.21899343  1.48596725  1.99143145  1.05458233  1.08250983]\n",
      " [-0.22318039  0.65017912  0.01410157 -0.16780589  0.36290753]]\n",
      "[[-0.16707033  1.17972157  0.14300413  0.26912336 -0.20601642]\n",
      " [ 0.08560938  1.15577836  1.97239442  1.27262803  0.03626041]\n",
      " [ 0.44334695  0.14352248 -0.30621817  0.09345263  1.19804751]]\n",
      "eval_numpy\n",
      "[[0.28282795 0.86633722 0.60257693 0.76006332 0.70923752]\n",
      " [0.18949885 1.31733272 1.21007392 0.71038713 0.43339924]\n",
      " [0.16221038 1.15390335 0.88341938 0.41306631 0.43839332]]\n",
      "[[0.29700547 0.79904247 0.53439649 0.74611433 0.55112404]\n",
      " [0.26163716 1.23206321 1.26263629 0.80220571 0.71174387]\n",
      " [0.15530735 0.91545366 0.60444962 0.57824888 0.63467022]]\n",
      "[[0.07649121 0.96214393 0.87319294 0.71938833 0.46180685]\n",
      " [0.15165504 0.95737192 1.48672289 1.03733326 0.52585261]\n",
      " [0.2580701  0.75562397 0.72253546 0.66372978 0.83297043]]\n",
      "eval_tensor\n",
      "[[0.28282795 0.86633722 0.60257693 0.76006332 0.70923752]\n",
      " [0.18949885 1.31733272 1.21007392 0.71038713 0.43339924]\n",
      " [0.16221038 1.15390335 0.88341938 0.41306631 0.43839332]]\n",
      "[[0.29700547 0.79904247 0.53439649 0.74611433 0.55112404]\n",
      " [0.26163716 1.23206321 1.26263629 0.80220571 0.71174387]\n",
      " [0.15530735 0.91545366 0.60444962 0.57824888 0.63467022]]\n",
      "[[0.07649121 0.96214393 0.87319294 0.71938833 0.46180685]\n",
      " [0.15165504 0.95737192 1.48672289 1.03733326 0.52585261]\n",
      " [0.2580701  0.75562397 0.72253546 0.66372978 0.83297043]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=8, suppress=True, linewidth=120)\n",
    "np.random.seed(123)\n",
    "torch.random.manual_seed(123)\n",
    "nums = 3\n",
    "\n",
    "x_numpy = np.array(np.random.random((nums, 3, 5)), dtype=np.float64)\n",
    "weight_numpy = np.array(np.random.random((5,)), dtype=np.float64)\n",
    "bias_numpy = np.array(np.random.random((5,)), dtype=np.float64)\n",
    "\n",
    "x_tensor = torch.tensor(x_numpy, requires_grad=True)\n",
    "weight_tensor = torch.tensor(weight_numpy, requires_grad=True)\n",
    "bias_tensor = torch.tensor(bias_numpy, requires_grad=True)\n",
    "\n",
    "batch_norm_numpy = BatchNorm1d(momentum=0.2)\n",
    "batch_norm_numpy.weight = weight_numpy\n",
    "batch_norm_numpy.bias = bias_numpy\n",
    "\n",
    "batch_norm_tensor = torch.nn.BatchNorm1d(5, momentum=0.2).double()\n",
    "batch_norm_tensor.weight = torch.nn.Parameter(weight_tensor, requires_grad=True)\n",
    "batch_norm_tensor.bias = torch.nn.Parameter(bias_tensor, requires_grad=True)\n",
    "\n",
    "print(\"output_numpy\")\n",
    "for i in range(nums):\n",
    "    output_numpy = batch_norm_numpy(x_numpy[i])\n",
    "    print(output_numpy)\n",
    "\n",
    "print(\"output_tensor\")\n",
    "for i in range(nums):\n",
    "    output_tensor = batch_norm_tensor(x_tensor[i])\n",
    "    print(output_tensor.data.numpy())\n",
    "\n",
    "print(\"eval_numpy\")\n",
    "batch_norm_numpy.train = False\n",
    "for i in range(nums):\n",
    "    output_numpy = batch_norm_numpy(x_numpy[i])\n",
    "    print(output_numpy)\n",
    "\n",
    "print(\"eval_tensor\")\n",
    "batch_norm_tensor.eval()\n",
    "for i in range(nums):\n",
    "    output_tensor = batch_norm_tensor(x_tensor[i])\n",
    "    print(output_tensor.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
